{"file_contents":{"ingest_sample.py":{"content":"# ingest_sample.py\nimport os\nfrom dotenv import load_dotenv\nfrom supabase import create_client\nfrom langchain_openai import OpenAIEmbeddings\n\nload_dotenv()\nSUPABASE_URL = os.environ[\"SUPABASE_URL\"]\nSUPABASE_KEY = os.environ[\"SERVICE_SUPABASESERVICE_KEY\"]  # use service role locally for writes\nEMBED_MODEL  = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\n\nsb = create_client(SUPABASE_URL, SUPABASE_KEY)\nemb = OpenAIEmbeddings(model=EMBED_MODEL)\n\n# two tiny example chunks for patient IVF001\nchunks = [\n    (\"Medication list: Letrozole 2.5 mg daily; Folic acid 5 mg.\", {\n        \"patient_id\":\"IVF001\",\"First_Name\":\"Priya\",\"Last_Name\":\"Sharma\",\"Date_of_birth\":\"1988-03-15\",\n        \"doc_id\":\"meds_2025.txt\"\n    }),\n    (\"Imaging: MRI pelvis 2025-09-14 shows adenomyosis; no adnexal mass.\", {\n        \"patient_id\":\"IVF001\",\"First_Name\":\"Priya\",\"Last_Name\":\"Sharma\",\"Date_of_birth\":\"1988-03-15\",\n        \"doc_id\":\"imaging_2025.txt\"\n    }),\n]\n\ntexts = [c[0] for c in chunks]\nvecs  = emb.embed_documents(texts)  # 1536-d each\n\nrows = []\nfor (text, md), v in zip(chunks, vecs):\n    rows.append({\"content\": text, \"metadata\": md, \"embedding\": v})\n\nresp = sb.table(\"rag_chunks\").insert(rows).execute()\nprint(\"Inserted:\", len(resp.data or []))\n","size_bytes":1242},"supabase_client.py":{"content":"import os\nfrom supabase import create_client, Client\nfrom dotenv import load_dotenv\n\nload_dotenv()\nSUPABASE_URL = os.getenv(\"SUPABASE_URL\")\nSUPABASE_KEY = os.getenv(\"SERVICE_SUPABASEANON_KEY\")\n\ndef get_supabase() -> Client:\n    if not SUPABASE_URL or not SUPABASE_KEY:\n        raise RuntimeError(\"Missing SUPABASE_URL or key.\")\n    return create_client(SUPABASE_URL, SUPABASE_KEY)\n","size_bytes":381},"retrieve_supabase.py":{"content":"from typing import List, Dict, Any\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_openai import OpenAIEmbeddings\nfrom supabase_client import get_supabase\n\nload_dotenv()\nEMBED_MODEL = os.getenv(\"EMBEDDING_MODEL\", \"text-embedding-3-small\")\nemb = OpenAIEmbeddings(model=EMBED_MODEL)\n\ndef match_patient_chunks(query: str, patient_id: str, k: int=6) -> List[Dict[str,Any]]:\n    \"\"\"\n    Calls SQL function:\n      match_patient_chunks(query_embedding vector, match_count int, p_patient_id text)\n    Returns rows: {id, content, metadata, similarity}\n    \"\"\"\n    sb = get_supabase()\n    qvec = emb.embed_query(query)\n    res = sb.rpc(\"match_patient_chunks_arr\", {\n        \"query_embedding\": qvec,\n        \"match_count\": k,\n        \"p_patient_id\": patient_id\n    }).execute()\n    return res.data or []\n","size_bytes":801},"replit.md":{"content":"# EHR Query Agent - Supabase Patient-Locked\n\n## Overview\nThis is a Streamlit-based Electronic Health Record (EHR) Query Agent that uses Retrieval-Augmented Generation (RAG) to provide AI-powered responses about patient medical records. The application connects to Supabase for data storage and uses OpenAI for embeddings and chat completion.\n\n## Project Purpose\nThe application allows healthcare professionals to:\n- Search and lock specific patient records securely\n- Ask natural language questions about patient medical information\n- Get AI-powered responses based on retrieved patient data\n- View sources for all AI responses for transparency\n\n## Recent Changes\n- **2025-10-23**: Initial setup in Replit environment\n  - Configured Python 3.11 environment\n  - Installed all dependencies from requirements.txt\n  - Set up Streamlit server on port 5000\n  - Configured deployment settings\n  - Added .gitignore and .env.example files\n\n## Project Architecture\n\n### Tech Stack\n- **Frontend**: Streamlit (Python web framework)\n- **Database**: Supabase (PostgreSQL with vector search)\n- **AI/ML**: \n  - OpenAI GPT-4o-mini for chat completion\n  - OpenAI text-embedding-3-small for semantic search\n  - LangChain for AI orchestration\n- **Search**: Vector similarity search using Supabase pgvector\n\n### File Structure\n```\n.\n├── app.py                   # Main Streamlit application\n├── patients.py              # Patient roster and fuzzy matching logic\n├── retrieve_supabase.py     # RAG retrieval functions\n├── supabase_client.py       # Supabase client initialization\n├── ingest_sample.py         # Sample data ingestion script\n├── requirements.txt         # Python dependencies\n├── .streamlit/\n│   └── config.toml         # Streamlit configuration\n├── .env.example            # Environment variables template\n└── .gitignore              # Git ignore rules\n```\n\n### Key Components\n\n1. **app.py**: Main application with patient selection and chat interface\n2. **patients.py**: Handles patient roster building and fuzzy name matching\n3. **retrieve_supabase.py**: Implements vector similarity search for RAG\n4. **supabase_client.py**: Supabase connection management\n5. **ingest_sample.py**: Utility script to ingest sample patient data\n\n## Configuration\n\n### Required Environment Variables\nThe following secrets are configured in Replit Secrets:\n- `OPENAI_API_KEY`: OpenAI API key for embeddings and chat\n- `SUPABASE_URL`: Your Supabase project URL\n- `SERVICE_SUPABASEANON_KEY`: Supabase anonymous key (for reads)\n- `SERVICE_SUPABASESERVICE_KEY`: Supabase service role key (for writes)\n\n### Optional Environment Variables\n- `LLM_MODEL`: OpenAI model to use (default: gpt-4o-mini)\n- `EMBEDDING_MODEL`: Embedding model (default: text-embedding-3-small)\n\n## Running the Application\n\n### Development\nThe Streamlit app runs automatically via the configured workflow on port 5000.\n\n### Deployment\nThe application is configured for autoscale deployment. Click the \"Deploy\" button in Replit to publish your app.\n\n## Database Schema\n\nThe application expects a Supabase table `rag_chunks` with the following structure:\n- `id`: Primary key\n- `content`: Text content of the medical record chunk\n- `metadata`: JSONB containing patient information\n- `embedding`: Vector(1536) for semantic search\n\nRequired SQL function: `match_patient_chunks_arr` for vector similarity search.\n\n## Usage Flow\n\n1. **Load Patient Roster**: App queries Supabase to build list of patients\n2. **Patient Selection**: User searches by ID or name (fuzzy matching supported)\n3. **DOB Verification**: User confirms patient identity with date of birth\n4. **Patient Lock**: System locks to selected patient for secure querying\n5. **Chat Interface**: User asks questions about the locked patient\n6. **RAG Retrieval**: System retrieves relevant chunks from Supabase\n7. **AI Response**: OpenAI generates response based on retrieved context\n8. **Source Display**: Sources are shown for transparency\n\n## Security Features\n\n- Patient verification via DOB before access\n- Patient-locked queries (can only query one patient at a time)\n- Environment variables for sensitive credentials\n- No exposure of API keys in code\n\n## Development Notes\n\n- The app uses Streamlit's session state to manage patient locks\n- Vector embeddings are 1536-dimensional (OpenAI text-embedding-3-small)\n- CORS and XSRF protection disabled for Replit proxy compatibility\n- Server runs on 0.0.0.0:5000 for proper Replit routing\n","size_bytes":4501},"app.py":{"content":"# app.py (streaming chat + Supabase RAG) — no duplicate assistant message\nimport os\nimport streamlit as st\nfrom dotenv import load_dotenv\nfrom langchain_openai import ChatOpenAI\nfrom patients import build_roster_from_supabase, fuzzy_resolve\nfrom retrieve_supabase import match_patient_chunks\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nLLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n\nif not OPENAI_API_KEY:\n    st.error(\"OPENAI_API_KEY missing in .env\")\n    st.stop()\n\nst.set_page_config(page_title=\"EHR Query Agent\", layout=\"centered\")\nst.markdown(\"## EHR Query Agent\")\n\nchat = ChatOpenAI(model=LLM_MODEL, temperature=0)\n\n# Load roster\nwith st.status(\"Patient data pre-loaded...\", expanded=False):\n    ROSTER = build_roster_from_supabase()\n\nif not ROSTER:\n    st.error(\"No patients found in Supabase rag_chunks.metadata.\")\n    st.stop()\n\n# Session state\nif \"locked\" not in st.session_state:\n    st.session_state.locked = None\nif \"pending\" not in st.session_state:\n    st.session_state.pending = None\nif \"messages\" not in st.session_state:\n    st.session_state.messages = []  # [{\"role\",\"content\",\"sources\"?}]\n\n# ───────────────── Patient selection UI ─────────────────\nst.markdown(\"### Select & Lock Patient\")\ncolA, colB = st.columns([3, 2])\nwith colA:\n    q = st.text_input(\"Patient ID or Name\", key=\"patient_query\")\nwith colB:\n    if st.button(\"Resolve\"):\n        resolved, candidates, reason = fuzzy_resolve(ROSTER, q)\n        if resolved:\n            st.session_state.pending = resolved\n            st.info(\n                f\"Detected: **{resolved['first_name']} {resolved['last_name']}** (`{resolved['patient_id']}`)\"\n            )\n        elif candidates:\n            st.session_state.pending = None  # Clear pending state\n            st.warning(\"Multiple matches:\")\n            for c in candidates:\n                st.write(\n                    f\"- `{c['patient_id']}` — {c['first_name']} {c['last_name']} (DOB {c['dob']})\"\n                )\n        else:\n            st.session_state.pending = None  # Clear pending state\n            st.error(\"No match found. Try again.\")\n\nif st.session_state.pending:\n    p = st.session_state.pending\n    dob_in = st.text_input(\"Confirm DOB (YYYY-MM-DD)\", key=\"dob_confirm\")\n    if st.button(\"Confirm patient\"):\n        if dob_in.strip() == (p[\"dob\"] or \"\").strip():\n            st.session_state.locked = p\n            st.session_state.pending = None\n            st.session_state.messages.clear()\n            st.success(\n                f\"Locked to patient: **{p['first_name']} {p['last_name']}** (`{p['patient_id']}`)\"\n            )\n        else:\n            st.error(\"DOB does not match.\")\n\nif st.session_state.locked:\n    lp = st.session_state.locked\n    st.success(\n        f\"**Locked:** {lp['first_name']} {lp['last_name']} (`{lp['patient_id']}`) — DOB: {lp['dob']}\"\n    )\nelse:\n    st.info(\"No patient locked yet.\")\n\n# ───────────────── Chat UI ─────────────────\nst.markdown(\"### Chat\")\n\n# 1) Render existing history FIRST (prevents duplicate of the streamed reply)\nfor msg in st.session_state.messages:\n    with st.chat_message(msg[\"role\"]):\n        st.write(msg[\"content\"])\n        if msg.get(\"sources\") and msg[\"role\"] == \"assistant\":\n            with st.expander(\"Sources\"):\n                st.json(msg[\"sources\"])\n\n# 2) Collect new input\nprompt = st.chat_input(\n    \"Ask about this patient (e.g., 'What medications is this patient on?')\")\n\n# 3) Handle the prompt after rendering history\nif prompt:\n    # append user msg\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    if not st.session_state.locked:\n        st.session_state.messages.append({\n            \"role\":\n            \"assistant\",\n            \"content\":\n            \"Please lock a patient first.\"\n        })\n        st.rerun()\n\n    # RAG for locked patient\n    pid = st.session_state.locked[\"patient_id\"]\n    with st.spinner(\"Retrieving context...\"):\n        hits = match_patient_chunks(prompt, pid, k=6)\n\n    context = [h[\"content\"] for h in hits]\n    sources = [h[\"metadata\"] for h in hits]\n    system = \"You are a clinical assistant. Use ONLY the retrieved patient context.\"\n    ctx_block = \"\\n---\\n\".join(context[:8]) if context else \"(no context)\"\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": system\n        },\n        {\n            \"role\": \"user\",\n            \"content\": f\"CONTEXT:\\n{ctx_block}\\n\\nQUESTION: {prompt}\\nAnswer:\"\n        },\n    ]\n\n    # 4) Stream the assistant reply LIVE (not added to history yet)\n    with st.chat_message(\"assistant\"):\n        placeholder = st.empty()\n        streamed = \"\"\n        with st.spinner(\"Thinking...\"):\n            for chunk in chat.stream(messages):\n                streamed += chunk.content or \"\"\n                placeholder.markdown(streamed)\n\n    # 5) Now persist the assistant message and refresh the app\n    st.session_state.messages.append({\n        \"role\": \"assistant\",\n        \"content\": streamed,\n        \"sources\": sources\n    })\n    st.rerun()\n","size_bytes":5145},"patients.py":{"content":"from typing import Dict, Any, List, Tuple, Optional\nfrom rapidfuzz import process, fuzz\nfrom supabase_client import get_supabase\n\nALIASES = {\n    \"patient_id\": [\"patient_id\",\"Patient_Id\",\"PatientID\"],\n    \"first_name\": [\"first_name\",\"First_Name\"],\n    \"last_name\": [\"last_name\",\"Last_Name\"],\n    \"dob\": [\"dob\",\"Date_of_birth\",\"DOB\"]\n}\n\ndef mget(md: Dict[str, Any], key: str) -> str:\n    for k in ALIASES.get(key, [key]):\n        if k in md:\n            return str(md[k])\n    return \"\"\n\ndef build_roster_from_supabase(limit: int = 20000) -> List[Dict[str, str]]:\n    sb = get_supabase()\n    res = sb.table(\"rag_chunks\").select(\"metadata\").limit(limit).execute()\n    by_pid = {}\n    for row in (res.data or []):\n        md = row.get(\"metadata\") or {}\n        pid = mget(md, \"patient_id\")\n        if not pid: continue\n        if pid not in by_pid:\n            by_pid[pid] = {\n                \"patient_id\": pid,\n                \"first_name\": mget(md, \"first_name\"),\n                \"last_name\": mget(md, \"last_name\"),\n                \"dob\": mget(md, \"dob\")\n            }\n    return list(by_pid.values())\n\ndef fuzzy_resolve(roster: List[Dict[str,str]], q: str) -> Tuple[Optional[Dict[str,str]], List[Dict[str,str]], str]:\n    q = (q or \"\").strip()\n    if not q: return None, [], \"none\"\n    id_hits = [r for r in roster if q.lower() in r[\"patient_id\"].lower()]\n    if len(id_hits)==1: return id_hits[0], [], \"by_id\"\n    if len(id_hits)>1: return None, id_hits, \"ambiguous\"\n    names = [f\"{r['first_name']} {r['last_name']}\".strip() for r in roster]\n    if names:\n        best = process.extractOne(q, names, scorer=fuzz.WRatio)\n        if best and best[1]>=80: return roster[best[2]], [], \"by_name\"\n        cands = process.extract(q, names, scorer=fuzz.WRatio, limit=5)\n        cands = [roster[idx] for name,score,idx in cands if score>=60]\n        if cands: return None, cands, \"ambiguous\"\n    return None, [], \"none\"\n","size_bytes":1912}},"version":2}